{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\McQue\\.conda\\envs\\GPU\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "PINECONE_API_HOST = os.environ.get('PINECONE_API_ENV')\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating the Index in Pinecone\n",
    "index_name = \"medical-chat-bot\"  # change if desired\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Configuring the pinecone index\n",
    "# import time\n",
    "# if index_name not in existing_indexes:\n",
    "#     pc.create_index(\n",
    "#         name=index_name,\n",
    "#         dimension=384,\n",
    "#         metric=\"cosine\",\n",
    "#         spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "#     )\n",
    "#     while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "#         time.sleep(1)\n",
    "\n",
    "# index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 2\n",
    "    )\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_split(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings(model_name):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name = model_name)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\McQue\\.conda\\envs\\GPU\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result = embeddings.embed_query(\"how are you\")\n",
    "# len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_pinecone import PineconeVectorStore\n",
    "# index_name = \"medical-chat-bot\"\n",
    "# docsearch = PineconeVectorStore.from_documents(text_chunks, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Document 1\n",
      "\n",
      "American Academy of Ophthalmology. 655 Beach Street, PO\n",
      "Box 7424, San Francisco, CA 94120-7424. <http://www.eyenet.org>.KEY TERMS\n",
      "Allergen —A substance capable of inducing an\n",
      "allergic response.\n",
      "Allergic reaction —An immune system reaction to\n",
      "a substance in the environment; symptomsinclude rash, inflammation, sneezing, itchy wateryeyes, and runny nose.\n",
      "Conjunctiva —The mucous membrane that covers\n",
      "the white part of the eyes and lines the eyelids.\n",
      "Edema —A condition where tissues contain exces-\n",
      "\n",
      "## Document 2\n",
      "\n",
      "aller-gies and hypersensitivity to foods, chemicals, and otheragents. Other tests for food allergies are the eliminationand rotation diets, in which foods are systematically eval-uated to determine the ones that are causing problems.\n",
      "\n",
      "## Document 3\n",
      "\n",
      "causes much more severe symptoms and generally afever. Allergies to molds or pollens also can make the\n",
      "nose run. Allergies are usually more persistent than thecommon cold. An allergist can do tests to determine ifthe cold-like symptoms are being caused by an allergicreaction. Also, some people get a runny nose when theygo outside in winter and breathe cold air. This type ofrunny nose is not a symptom of a cold.\n",
      "Treatment\n",
      "There are no medicines that will cure the common\n",
      "\n",
      "## Document 4\n",
      "\n",
      "cates lack of T cell responsiveness, a condition calledanergy. T cell anergy is seen in immune deficiencydiseases including AIDS , some cases of infectious\n",
      "diseases, malignancies, immunosuppressive therapy(including corticosteroid treatment), some autoim-mune diseases, malnutrition, major surgery, and someviral immunizations.\n",
      "Resources\n",
      "BOOKS\n",
      "Davies, R., and S. Ollier. Allergy: The Facts. Oxford University\n",
      "Press, 1989.\n",
      "Lawlor Jr., G. J., et. al. Manual of Allergy and Immunology.\n"
     ]
    }
   ],
   "source": [
    "query = \"what are allergies\"\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "matched_docs = retriever.invoke(query)\n",
    "for i, d in enumerate(matched_docs):\n",
    "    print(f\"\\n## Document {i+1}\\n\")\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Use the following pieces of information to answer the user's question. If you don't know the answer, just say you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model=r\"../model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                    model_type=\"llama\",\n",
    "                    config={\"max_new_tokens\" : 512,\n",
    "                            'temperature': 0.2}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='## Attention in Generative AI: Focusing on What Matters\\n\\nIn the world of Generative AI, \"attention\" is a powerful mechanism that mimics how humans focus on specific parts of information when processing it. Just like we pay more attention to certain words in a sentence to understand its meaning, attention mechanisms in AI models help them focus on the most relevant parts of the input data to generate better outputs.\\n\\nHere\\'s a breakdown:\\n\\n**Imagine you\\'re translating a sentence:** \"The cat sat on the mat, which was blue.\"\\n\\n* **Without attention:** A basic model might process the entire sentence equally, potentially getting confused by the \"which was blue\" clause when translating \"the cat.\"\\n* **With attention:** The model would learn to pay more \"attention\" to the words directly related to \"the cat\" (like \"sat\" and \"mat\"), effectively ignoring the less relevant parts for that translation step.\\n\\n**How does it work?**\\n\\nAttention mechanisms utilize a system of **weights** to determine the importance of different parts of the input. These weights are learned during the AI model\\'s training process. Higher weights signify higher importance, allowing the model to prioritize those parts when generating the output.\\n\\n**Types of Attention:**\\n\\n* **Self-attention:**  The model focuses on different parts of the *same* input sequence. This is crucial for understanding long-range dependencies in text, like how the word \"it\" in a sentence refers to something mentioned earlier.\\n* **Multi-head attention:**  The model uses multiple attention \"heads\" to focus on different aspects of the input simultaneously. This allows for a richer understanding of the relationships between words and concepts.\\n\\n**Benefits of Attention in Generative AI:**\\n\\n* **Improved performance:**  By focusing on the most relevant information, attention mechanisms lead to more accurate and coherent outputs in tasks like:\\n    * Machine translation\\n    * Text summarization\\n    * Image captioning\\n    * Speech recognition\\n* **Handling long sequences:** Attention helps models process longer pieces of text or data without losing track of important information.\\n* **Interpretability:**  Analyzing the attention weights can provide insights into the model\\'s decision-making process, making it easier to understand why it generated a specific output.\\n\\n**Examples of Attention in Action:**\\n\\n* **Google Translate:** Uses attention mechanisms to significantly improve translation quality, especially for longer sentences.\\n* **GPT-3 (Generative Pre-trained Transformer 3):** Relies heavily on multi-head attention to generate human-like text in various creative writing tasks.\\n\\n**In conclusion,** attention is a fundamental concept in Generative AI, allowing models to process information more effectively and generate higher-quality outputs. Its ability to mimic human-like focus makes it a crucial component in building more sophisticated and intelligent AI systems. \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-1783d7f0-858b-43f9-bd6c-b55ce137ee61-0', usage_metadata={'input_tokens': 7, 'output_tokens': 581, 'total_tokens': 588})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is attention in genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm = llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever = vectorstore.as_retriever(search_kwarg = {'k': 2}),\n",
    "                                 return_source_documents = True,\n",
    "                                 chain_type_kwargs= chain_type_kwargs\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoked = qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don's:\n",
      "I can beacute:\n",
      "The American Academy of the user's:\n",
      "I don's:\n",
      "I amou could include:\n",
      "I's:\n",
      "I don's:\n",
      "I's\n",
      "I don's:\n",
      "I don's:\n",
      "I don's are available.\n",
      "I can be given by Tips for the user's:\n",
      "The American Academy of the user's\n",
      "I don's:\n",
      "I's:\n",
      "I can beacute:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "The American Academy of the user's:\n",
      "I can beacute:\n",
      "\n",
      "I's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's\n",
      "I don's:\n",
      "I amd:\n",
      "I don's\n",
      "I's:\n",
      "I don's:\n",
      "The American Academy of the user's:\n",
      "I don's:\n",
      "I can beacute:\n",
      "I's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I's:\n",
      "I don's:\n",
      "I don's:\n",
      "The American Academy of the user's\n",
      "I don's:\n",
      "I can beacute:\n",
      "I's:\n",
      "I don's:\n",
      "I's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I cannot be given by Suggestions:\n",
      "The American Academy of the user's:\n",
      "I can beacute:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I don's:\n",
      "I's:\n",
      "I don's:\n",
      "I don's\n",
      "I don's:\n",
      "I don's:\n",
      "\n",
      "\n",
      "The American Academy of the user's:\n",
      "I can beacneeds:\n",
      "I don's:\n",
      "I's:\n",
      "I don's:\n",
      "I don's\n",
      "I's:\n",
      "I don's\n",
      "I don's:\n",
      "I amd:\n",
      "I's:\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "print(invoked['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: what are acne \n",
      "answer: I'm sorry, but the provided context does not contain information about acne. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: what are acne \n",
      "answer: The provided text does not contain the answer to what acne is. \n",
      "\n",
      "Exiting....\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\McQue\\.conda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "while True:\n",
    "    user_input = input(f'input Prompt: ')\n",
    "    if user_input == 'exit':\n",
    "        print('Exiting....')\n",
    "        sys.exit()\n",
    "    if user_input == '':\n",
    "        continue\n",
    "    result = qa.invoke({'query': user_input})\n",
    "    print(f\"query: {result['query']} \\nanswer: {result['result']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
